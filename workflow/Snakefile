import sys
from pathlib import Path

import pandas as pd

from snakemake.utils import Paramspace
from snakemake.io import Namedlist

# setup config
configfile: "config/config.yaml"

fname_reference = config["fname_reference"]
fname_insert_bed = config["fname_insert_bed"]
fname_samples = config["fname_samples"]
dir_path_samples = config["dir_path_samples"]

df_samples = pd.read_csv(fname_samples, comment="#")
all_samples = [f"{row[2]}/{row[3]}" for row in df_samples.itertuples()]

# rule definitions
rule all:
    input:
        f"results/mutations_of_interest.csv",
        f"results/all_mutations.csv",
        f"results/all_haplotypes.fasta",
        f"results/all_mutations.annotated.csv", # note that here deletions are excluded

rule copy_coverage_file:
    input:
        fname_coverage=dir_path_samples+f"{{sample}}/alignments/coverage.tsv.gz",
    output:
        fname_coverage_zipped=f"results/{{sample}}/alignment/coverage.tsv.gz",
    shell:
        """
        cp {input.fname_coverage} {output.fname_coverage_zipped}
        """

rule unzip_coverage_file:
    input:
        fname_coverage_zipped=f"results/{{sample}}/alignment/coverage.tsv.gz",
    output:
        fname_coverage=f"results/{{sample}}/alignment/coverage.tsv",
    shell:
        """
        gunzip {input.fname_coverage_zipped}
        """

rule check_coverage:
    input:
        fnames_coverage=[
            f"results/{sample}/alignment/coverage.tsv"
            for sample in all_samples
        ],
    output:
        fname_samples=f"results/samples.processsed.csv",
        fname_bad_cov_samples=f"results/samples.bad_coverage.csv",
    params:
        samples=all_samples,
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/check_coverage.py"


rule provide_alignment:
    input:
        fname_bam=dir_path_samples+f"{{sample}}/alignments/REF_aln.bam",
        fname_bam_idx=dir_path_samples+f"{{sample}}/alignments/REF_aln.bam.bai",
    output:
        fname_bam=f"results/{{sample}}/alignment/REF_aln.bam",
        fname_bam_idx=f"results/{{sample}}/alignment/REF_aln.bam.bai",
    shell:
        """
        cp {input.fname_bam} {output.fname_bam}
        cp {input.fname_bam_idx} {output.fname_bam_idx}
        """

rule split:
    conda:
        "envs/split.yaml"
    input:
        fname_bam=f"results/{{sample}}/alignment/REF_aln.bam",
    output:
        fname_fastq_R1=f"results/{{sample}}/alignment/reads.R1.fastq",
        fname_fastq_R2=f"results/{{sample}}/alignment/reads.R2.fastq",
        #fname_fastq_s=f"results/{{sample}}/alignment/reads.singletones.fastq",
    shell:
        "samtools fastq {input.fname_bam} -1 {output.fname_fastq_R1} -2 {output.fname_fastq_R2}"


rule flash:
    input:
        fname_fastq_R1=f"results/{{sample}}/alignment/reads.R1.fastq",
        fname_fastq_R2=f"results/{{sample}}/alignment/reads.R2.fastq",
    output:
        fname_fastq_merged=f"results/{{sample}}/alignment/flash/reads_merged.extendedFrags.fastq",
        dname_out=directory(
            f"results/{{sample}}/alignment/flash/"
        ),
    conda:
        "envs/split.yaml"
    shell:
        """
        flash \
          {input.fname_fastq_R1} \
          {input.fname_fastq_R2} \
          -r 200 \
          -f 400 \
          -s 60 \
          -x 0.001 \
          -m 1 \
          --allow-outies \
          --output-prefix=reads_merged \
          --output-directory={output.dname_out}
        """

rule alignment_merged:
    input:
        fname_reference=fname_reference,
        fname_fastq_merged=f"results/{{sample}}/alignment/flash/reads_merged.extendedFrags.fastq",
    output:
        fname_bam=f"results/{{sample}}/alignment/REF_aln.merged.bam",
    conda:
        "envs/split.yaml"
    shell:
        """
        bwa index {input.fname_reference}
        bwa mem {input.fname_reference} {input.fname_fastq_merged} > {output.fname_bam}
        samtools sort -o {output.fname_bam} {output.fname_bam}
        """

rule subsample_alignment:
    input:
        fname_bam=f"results/{{sample}}/alignment/REF_aln.merged.bam",
    output:
        fname_bam_unsort=temp(f"results/{{sample}}/alignment/REF_aln.merged.unsort.bam"),
        fname_bam_unsort_sub=temp(f"results/{{sample}}/alignment/REF_aln.merged.unsort.subsampled.bam"),
        fname_bam=f"results/{{sample}}/alignment/REF_aln.merged.subsampled.bam",
    params:
        coverage_threshold=20000,
    conda:
        "envs/subsampling.yaml"
    shell:
        """
        jvarkit sortsamrefname --samoutputformat BAM {input.fname_bam} > {output.fname_bam_unsort}

        jvarkit biostar154220 -n {params.coverage_threshold} --samoutputformat BAM {output.fname_bam_unsort} > {output.fname_bam_unsort_sub}

        samtools sort -o {output.fname_bam}  {output.fname_bam_unsort_sub}
        """


rule remove_original_aln:
    input:
        fname_bam_merged=f"results/{{sample}}/alignment/REF_aln.merged.bam",
        fname_bam=f"results/{{sample}}/alignment/REF_aln.bam",
        fname_bam_idx=f"results/{{sample}}/alignment/REF_aln.bam.bai",
    shell:
        """
        rm {input.fname_bam}
        rm {input.fname_bam_idx}
        """

rule run_viloca:
    input:
        fname_bam=f"results/{{sample}}/alignment/REF_aln.merged.subsampled.bam",
        #fname_bam=rules.alignment_merged.output.fname_bam,
        fname_reference=fname_reference,
        fname_insert_bed=fname_insert_bed,
        fname_bad_samples= f"results/samples.bad_coverage.csv",
    output:
        fname_vcf=f"results/{{sample}}/snvs.vcf",
        dname_work=directory(
            f"results/{{sample}}/variant_calling/"
        ),
        dname_haplotypes=directory(
            f"results/{{sample}}/variant_calling/support/"
        ),
    benchmark:
        f"results/{{sample}}/benchmark.tsv"
    params:
        sample= lambda wc: wc.get("sample")
    conda:
        "envs/viloca.yaml"
    threads: 15,
    resources:
        mem_mb=10000,
        runtime=4*24*60, # 4 days
    script:
        "scripts/run_viloca.py"


rule get_haplotypes:
    input:
        dname_haplotypes=directory(
            f"results/{{sample}}/variant_calling/support/"
        ),
    output:
        fname_haplotypes=f"results/{{sample}}/haplotypes.fasta",
    params:
        sample= lambda wc: wc.get("sample")
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "scripts/haplotypes.py"

rule collect_haplotypes:
    input:
        haplos=[
            f"results/{sample}/haplotypes.fasta"
            for sample in all_samples
        ],
    output:
        fname=f"results/all_haplotypes.fasta",
    script:
        "scripts/combine_txt_files.py"



rule annotate_vcf:
    input:
        fname_snvs_vcf=f"results/{{sample}}/snvs.vcf",
        fname_genbank_file="resources/GCF_009858895.2_ASM985889v3_genomic.gbff",
    output:
        fname_snvs_vcf=f"results/{{sample}}/snvs.annotated.vcf",
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/annotate_vcf.py"


rule collect_all_annotated_mutations:
    input:
        vcf_list=[
            f"results/{sample}/snvs.annotated.vcf"
            for sample in all_samples
        ],
    output:
        fname_result_csv=f"results/all_mutations.annotated.csv",
    params:
        params=all_samples,
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/collect_all_annotated_mutations.py"

rule mutations_of_interest:
    input:
        fname_all_mutations=f"results/all_mutations.annotated.csv",
        fname_bad_samples=f"results/samples.bad_coverage.csv",
    params:
        fname_mutation_definitions=f"resources/mutation_definitions.yaml",
        all_samples=all_samples,
    output:
        fname_mutations=f"results/mutations_of_interest.csv",
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/scan_mutations.py"


rule collect_all_mutations:
    input:
        vcf_list=[
            f"results/{sample}/snvs.vcf"
            for sample in all_samples
        ],
    output:
        fname_result_csv=f"results/all_mutations.csv",
    params:
        params=all_samples,
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/collect_all_annotated_mutations.py"
